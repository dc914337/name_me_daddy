{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('äåöjkjh jhjk j1hkj', 'jfjh jhjk ef jhkj', 'jkjh jhjk jhkj')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import regex as re\n",
    "\n",
    "def normalize_text(text):\n",
    "    clean_text=text.lower()\n",
    "    clean_text=re.sub(r\"[^a-zA-Z0-9\\säåö]\",\" \",clean_text)\n",
    "    #clean_text=re.sub(r\"[^\\S\\n]+\",\" \",clean_text)\n",
    "    clean_text=re.sub(r\"\\s+\",\" \",clean_text)\n",
    "    clean_text=re.sub(r\"^\\s\",\"\",clean_text)\n",
    "    clean_text=re.sub(r\"\\s$\",\"\",clean_text)\n",
    "    return clean_text\n",
    "\n",
    "normalize_text(\"äåöjkjh jhjk j1hkj\"),normalize_text(\"jFjh.jhjk,.,.ef jhkj \"),normalize_text(\" jkjh jhjk jhkj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "finnish_company_names=[]\n",
    "with open('fullprhdata.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "    for row in csv_reader:\n",
    "        finnish_company_names.append(row[0])\n",
    "json.dump(finnish_company_names,open(\"finnish_registry_companies.json\",\"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487e1dd294694f03a4abbfa187b6d52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=330538.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import random\n",
    "finnish_companies = json.load(open(\"finnish_registry_companies.json\",\"r+\"))\n",
    "all_companies = json.load(open(\"all_kaggle_companies.json\",\"r+\"))\n",
    "\n",
    "names_text=\"\"\n",
    "\n",
    "# companies \n",
    "#names_arr=[normalize_text(company) for company in tqdm(all_companies)]\n",
    "#random.shuffle(names_arr)\n",
    "#names_text=\"\\n\".join(names_arr)\n",
    "\n",
    "# finnish \n",
    "finnish_names_arr=[normalize_text(company) for company in tqdm(finnish_companies)]\n",
    "random.shuffle(finnish_names_arr)\n",
    "finnish_names_text=\"\\n\".join(finnish_names_arr)\n",
    "\n",
    "\n",
    "characters=sorted(list(set(names_text+finnish_names_text)))\n",
    "characters=[\".\"]+characters # . is none\n",
    "char_indices = dict((c, i) for i, c in enumerate(characters))\n",
    "indices_char = dict((i, c) for i, c in enumerate(characters))\n",
    "\n",
    "Tx=25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: .\n",
      " 0123456789abcdefghijklmnopqrstuvwxyzäåö\n",
      "Chars:  42\n",
      "Tx:  25\n"
     ]
    }
   ],
   "source": [
    "print(\"Characters:\",\"\".join(characters))\n",
    "print(\"Chars: \",len(characters))\n",
    "print(\"Tx: \",Tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".hello world\n"
     ]
    }
   ],
   "source": [
    "def str_to_vec(word, start_with_null=False):\n",
    "    \"\"\"\n",
    "    Converts word to vec\n",
    "    \n",
    "    word -- string\n",
    "    \n",
    "    returns array of shape (Tx, len(chars))\n",
    "    \"\"\"\n",
    "    if start_with_null:\n",
    "        word=\".\"+word\n",
    "    x = np.zeros((len(word), len(characters)), dtype=np.bool)\n",
    "    for t, char in enumerate(word):\n",
    "        x[t, char_indices[char]] = 1\n",
    "    return x\n",
    "    \n",
    "def vec_to_str(vec):\n",
    "    \"\"\"\n",
    "    Converts vec to word\n",
    "    \n",
    "    vec -- array of shape (Tx, len(chars))\n",
    "    \n",
    "    \"\"\"\n",
    "    word=\"\"\n",
    "    for i in range(vec.shape[0]):\n",
    "        word+=indices_char[np.argmax(vec[i])]\n",
    "    return word\n",
    "\n",
    "a=str_to_vec(\"hello world\",start_with_null=True)\n",
    "#print(a)\n",
    "print(vec_to_str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-473f21992f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3430\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvec_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Xf' is not defined"
     ]
    }
   ],
   "source": [
    "id_=3430\n",
    "vec_to_str(Xf[id_]),vec_to_str(np.array(Yf[id_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(characters)), p = probas.ravel())\n",
    "    out=characters[out]\n",
    "    return out\n",
    "\n",
    "def generate_output(model, text_start, length=5):\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(f\"Diversity: {diversity}\")\n",
    "        for l in range(length):\n",
    "            generated = '\\n'\n",
    "            sentence = ('{0:\\n>' + str(Tx) + '}').format(text_start).lower()\n",
    "            generated += text_start \n",
    "            gens=0\n",
    "\n",
    "            generated = \"\"\n",
    "            inp=text_start\n",
    "            sentence = inp\n",
    "\n",
    "            x_pred = np.zeros((1, Tx, len(characters)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            for pred in preds:\n",
    "                next_index = sample(pred)\n",
    "                next_char = next_index\n",
    "                sentence = sentence + next_char\n",
    "                generated += next_char\n",
    "            print(f\"{inp}{generated}\")\n",
    "        \n",
    "generate_output(model,\"accent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Dense, Input, LSTM,GRU\n",
    "\n",
    "drp=0\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(128, input_shape=(Tx, len(characters)),return_sequences=True))\n",
    "model.add(Dense(len(characters),activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    generate_output(model,\"acc\")\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "history=model.fit(X, Y, batch_size=4096, validation_split=0.1,  epochs=100, shuffle=True, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"alltext.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per name model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import random\n",
    "finnish_companies = json.load(open(\"finnish_registry_companies.json\",\"r+\"))\n",
    "\n",
    "Tx=15\n",
    "\n",
    "# finnish \n",
    "finnish_names_arr=[normalize_text(company) for company in finnish_companies if len(company)<Tx]\n",
    "random.shuffle(finnish_names_arr)\n",
    "finnish_names_text=\"\".join(finnish_names_arr)\n",
    "\n",
    "characters=sorted(list(set(finnish_names_text)))\n",
    "characters=[\".\",\"E\"]+characters # . is none, E is end of line\n",
    "char_indices = dict((c, i) for i, c in enumerate(characters))\n",
    "indices_char = dict((i, c) for i, c in enumerate(characters))\n",
    "\n",
    "print(len(finnish_names_arr))\n",
    "print(\"\\n\".join(random.choices(finnish_names_arr,k=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(words, n_x, Tx=None):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, len(chars))\n",
    "    y -- array of shape (m, Tx, len(chars))\n",
    "    \"\"\"\n",
    "    if Tx is None:\n",
    "        Tx=len(max(words,key=len))\n",
    "        print(Tx)\n",
    "    \n",
    "    m = len(words)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    \n",
    "    for w, word in enumerate(tqdm(words)):\n",
    "        word=word[:Tx]\n",
    "        x[w, 0:len(word)+1, :] = str_to_vec(word,start_with_null=True)\n",
    "        x[w, len(word)+1:, char_indices[\"E\"]] = 1\n",
    "        \n",
    "        y[w, 0:len(word),:] = str_to_vec(word)\n",
    "        y[w, len(word):, char_indices[\"E\"]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "Xf,Yf=vectorization(finnish_names_arr, n_x=len(characters), Tx=Tx)\n",
    "Xf.shape, Yf.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(vec_to_str(Xf[11324])),\n",
    "print(vec_to_str(Yf[11324]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_name(model, beginning=\"\"):\n",
    "    name = beginning\n",
    "    x = np.zeros((1, Tx, len(characters)))\n",
    "    x[0,0:len(beginning),:]=str_to_vec(beginning)\n",
    "    \n",
    "    for i in range(len(beginning)-1,Tx-1):\n",
    "        prediction=model.predict(x)[0]\n",
    "        probs = list(prediction[i])\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.random.choice(range(len(characters)), p=probs)\n",
    "        #index = np.argmax(probs)\n",
    "        print(index)\n",
    "        character = indices_char[index]\n",
    "        print(f\"{vec_to_str(x[0])} -> {vec_to_str(prediction)} -> {character}\")\n",
    "        if character==\"E\":\n",
    "            break\n",
    "        name+=character\n",
    "        x[0, i+1, index] = 1\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print(name)\n",
    "\n",
    "make_name(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.pad_sequences([[0,0,0],[0,0,0],[0,0,0]], maxlen=10, value=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Dense, Input, LSTM,GRU\n",
    "\n",
    "drp=0\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(64, input_shape=(Tx, len(characters)), return_sequences=True))\n",
    "model.add(Dense(len(characters),activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    for i in range(3):\n",
    "        make_name(model,\"em\")\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "history=model.fit(Xf, Yf, batch_size=8192, validation_split=0.1,  epochs=1000, shuffle=True, callbacks=[print_callback],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "results=history\n",
    "\n",
    "plt.figure(figsize=(8, 16))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "empty=\".\"\n",
    "eos=\"E\"\n",
    "eos_index=char_indices[eos]\n",
    "xn=len(characters)\n",
    "\n",
    "\n",
    "def predict_tree(x, position, k):\n",
    "    pred=model.predict(x)[0][position]\n",
    "    indices = pred.argsort()[-k:]\n",
    "    results=np.zeros((0, Tx, xn))\n",
    "    for index in indices:\n",
    "        res=np.array(x, copy=True)\n",
    "        res[0,position,index]=pred[index]\n",
    "        if index==eos_index:\n",
    "            results=np.append(results,res,axis=0)\n",
    "            break\n",
    "        if position==Tx-1:\n",
    "            results=np.append(results,res,axis=0)\n",
    "            break\n",
    "        results=np.append(results,predict_tree(res,position+1,k), axis=0)\n",
    "    return results\n",
    "\n",
    "\n",
    "def beamsearch(k, cnt):\n",
    "    x = np.zeros((1, Tx, xn))\n",
    "    res=predict_tree(x, 0, k)\n",
    "    probs=np.sum(res,axis=(1,2))\n",
    "    samples=[vec_to_str(a) for a in np.take(res,np.argsort(probs),axis=0)[-k:]]\n",
    "    return list(zip(probs,samples))\n",
    "    \n",
    "    \n",
    "res=beamsearch(k=2,cnt=10)\n",
    "for prob, sample in res:\n",
    "    print(f\"{probs:0.2f}: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "# train language model for the beam search\n",
    "# end of line character should be zeros\n",
    "# dropout\n",
    "# try GRU\n",
    "# save best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Dense, Input, LSTM,GRU\n",
    "\n",
    "drp=0\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(64, input_shape=(Tx, len(characters)),return_sequences=True,activation=\"softmax\"))\n",
    "model.add(Dense(len(characters),activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(Xf[2:3,:,:]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xf[:1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvognition",
   "language": "python",
   "name": "kvognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
