{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('äåöjkjh jhjk j1hkj', 'jfjh jhjk ef jhkj', 'jkjh jhjk jhkj')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import regex as re\n",
    "\n",
    "def normalize_text(text):\n",
    "    clean_text=text.lower()\n",
    "    clean_text=re.sub(r\"[^a-zA-Z0-9\\säåö]\",\" \",clean_text)\n",
    "    #clean_text=re.sub(r\"[^\\S\\n]+\",\" \",clean_text)\n",
    "    clean_text=re.sub(r\"\\s+\",\" \",clean_text)\n",
    "    clean_text=re.sub(r\"^\\s\",\"\",clean_text)\n",
    "    clean_text=re.sub(r\"\\s$\",\"\",clean_text)\n",
    "    return clean_text\n",
    "\n",
    "normalize_text(\"äåöjkjh jhjk j1hkj\"),normalize_text(\"jFjh.jhjk,.,.ef jhkj \"),normalize_text(\" jkjh jhjk jhkj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "finnish_company_names=[]\n",
    "with open('fullprhdata.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "    for row in csv_reader:\n",
    "        finnish_company_names.append(row[0])\n",
    "json.dump(finnish_company_names,open(\"finnish_registry_companies.json\",\"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provendix oy\n",
      "trakai oy\n",
      "zerochaos oy\n",
      "sanmet oy\n",
      "oy el ho ab\n",
      "\n",
      "Found names: 72385\n",
      "Characters: .E 0123456789abcdefghijklmnopqrstuvwxyzäåö\n",
      "Chars:  42\n",
      "Tx:  13\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import random\n",
    "finnish_companies = json.load(open(\"finnish_registry_companies.json\",\"r+\"))\n",
    "\n",
    "Tx=13\n",
    "\n",
    "# finnish \n",
    "finnish_names_arr=[normalize_text(company) for company in finnish_companies if len(company)<Tx]\n",
    "random.shuffle(finnish_names_arr)\n",
    "finnish_names_text=\"\".join(finnish_names_arr)\n",
    "\n",
    "characters=sorted(list(set(finnish_names_text)))\n",
    "characters=[\".\",\"E\"]+characters # . is none, E is end of line\n",
    "char_indices = dict((c, i) for i, c in enumerate(characters))\n",
    "indices_char = dict((i, c) for i, c in enumerate(characters))\n",
    "\n",
    "print(\"\\n\".join(random.choices(finnish_names_arr,k=5)))\n",
    "print()\n",
    "print(f\"Found names:\", len(finnish_names_arr))\n",
    "print(\"Characters:\",\"\".join(characters))\n",
    "print(\"Chars: \",len(characters))\n",
    "print(\"Tx: \",Tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def str_to_vec(word, start_with_null=False):\n",
    "    \"\"\"\n",
    "    Converts word to vec\n",
    "    \n",
    "    word -- string\n",
    "    \n",
    "    returns array of shape (Tx, len(chars))\n",
    "    \"\"\"\n",
    "    if start_with_null:\n",
    "        word=\".\"+word\n",
    "    x = np.zeros((len(word), len(characters)), dtype=np.bool)\n",
    "    for t, char in enumerate(word):\n",
    "        x[t, char_indices[char]] = 1\n",
    "    return x\n",
    "    \n",
    "def vec_to_str(vec, debug=False):\n",
    "    \"\"\"\n",
    "    Converts vec to word\n",
    "    \n",
    "    vec -- array of shape (Tx, len(chars))\n",
    "    \n",
    "    \"\"\"\n",
    "    word=\"\"\n",
    "    for i in range(vec.shape[0]):\n",
    "        word+=indices_char[np.argmax(vec[i])]\n",
    "    \n",
    "    if debug==False:\n",
    "        word=word.replace(\"E\",\"\").replace(\".\",\"\")\n",
    "    return word\n",
    "\n",
    "a=str_to_vec(\"hello world\",start_with_null=True)\n",
    "print(vec_to_str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6842c4532249fc95d9bb73bb19e16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=72385.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((72385, 13, 42), (72385, 13, 42))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorization(words, n_x, Tx=None):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, len(chars))\n",
    "    y -- array of shape (m, Tx, len(chars))\n",
    "    \"\"\"\n",
    "    if Tx is None:\n",
    "        Tx=len(max(words,key=len))\n",
    "        print(Tx)\n",
    "    \n",
    "    m = len(words)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    \n",
    "    for w, word in enumerate(tqdm(words)):\n",
    "        word=word[:Tx]\n",
    "        x[w, 0:len(word)+1, :] = str_to_vec(word,start_with_null=True)\n",
    "        x[w, len(word)+1:, char_indices[\"E\"]] = 1\n",
    "        \n",
    "        y[w, 0:len(word),:] = str_to_vec(word)\n",
    "        y[w, len(word):, char_indices[\"E\"]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "X,Y=vectorization(finnish_names_arr, n_x=len(characters), Tx=Tx)\n",
    "X.shape, Y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".minimer oyEE\n",
      "minimer oyEEE\n",
      "\n",
      ".lhmed oyEEEE\n",
      "lhmed oyEEEEE\n",
      "\n",
      ".aher oyEEEEE\n",
      "aher oyEEEEEE\n",
      "\n",
      ".destaplan oy\n",
      "destaplan oyE\n",
      "\n",
      ".b1g lahti oy\n",
      "b1g lahti oyE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_=3430\n",
    "for id_ in random.choices(range(X.shape[0]),k=5):\n",
    "    print(vec_to_str(X[id_]))\n",
    "    print(vec_to_str(np.array(Y[id_])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 0.2\n",
      "ghai tEEEEEEE\n",
      "uclaeeEEEEEEE\n",
      "msjio EEEEEEE\n",
      "arlmatEEEEEEE\n",
      "mmatieEEEEEEE\n",
      "Diversity: 0.5\n",
      "eulua EEEEEEE\n",
      "wcia  EEEEEEE\n",
      "jkse aEEEEEEE\n",
      "mdt ttEEEEEEE\n",
      "nn ou EEEEEEE\n",
      "Diversity: 1.0\n",
      "cmzta EEEEEEE\n",
      "kart yEEEEEEE\n",
      "mnro  EEEEEEE\n",
      "tmie aEEEEEEE\n",
      "tjsoeaEEEEEEE\n",
      "Diversity: 1.2\n",
      "krewtuEEEEEEE\n",
      "bta a EEEEEEE\n",
      "yaroo EEEEEEE\n",
      "1eeie EEEEEEE\n",
      "it e eEEEEEEE\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(characters)), p = probas.ravel())\n",
    "    out=characters[out]\n",
    "    return out\n",
    "\n",
    "def generate_output(model, text_start, length=5):\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(f\"Diversity: {diversity}\")\n",
    "        for l in range(length):\n",
    "            generated = '\\n'\n",
    "            sentence = ('{0:\\n>' + str(Tx) + '}').format(text_start).lower()\n",
    "            generated += text_start \n",
    "            gens=0\n",
    "\n",
    "            generated = \"\"\n",
    "            inp=text_start\n",
    "            sentence = inp\n",
    "\n",
    "            x_pred = np.zeros((1, Tx, len(characters)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            for pred in preds:\n",
    "                next_index = sample(pred)\n",
    "                next_char = next_index\n",
    "                sentence = sentence + next_char\n",
    "                generated += next_char\n",
    "            print(f\"{inp}{generated}\")\n",
    "        \n",
    "generate_output(model,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one oy\n"
     ]
    }
   ],
   "source": [
    "def make_name(model, beginning=\"\"):\n",
    "    name = beginning\n",
    "    x = np.zeros((1, Tx, len(characters)))\n",
    "    x[0,0:len(beginning),:]=str_to_vec(beginning,start_with_null=False)\n",
    "\n",
    "    for i in range(len(beginning),Tx-1):\n",
    "        prediction=model.predict(x)[0]\n",
    "        probs = list(prediction[i])\n",
    "        probs = probs / np.sum(probs)\n",
    "        #index = np.random.choice(range(len(characters)), p=probs)\n",
    "        index = np.argmax(prediction[i])\n",
    "        character = indices_char[index]\n",
    "        #print(f\"{vec_to_str(x[0])} -> {vec_to_str(prediction)} -> {character}\")\n",
    "        if character==\"E\":\n",
    "            break\n",
    "        name+=character\n",
    "        x[0, i+1, index] = 1\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print(name)\n",
    "\n",
    "make_name(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 13, 64)            27392     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13, 42)            2730      \n",
      "=================================================================\n",
      "Total params: 30,122\n",
      "Trainable params: 30,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Dense, Input, LSTM,GRU\n",
    "\n",
    "drp=0\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(64, input_shape=(Tx, len(characters)), return_sequences=True))\n",
    "model.add(Dense(len(characters),activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    for i in range(3):\n",
    "        make_name(model,\"\")\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "history=model.fit(Xf, Yf, batch_size=64, validation_split=0.1,  epochs=1000, shuffle=True, callbacks=[print_callback],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "results=history\n",
    "\n",
    "plt.figure(figsize=(8, 16))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "empty=\".\"\n",
    "eos=\"E\"\n",
    "eos_index=char_indices[eos]\n",
    "xn=len(characters)\n",
    "\n",
    "\n",
    "def predict_tree(x, position, k):\n",
    "    prediction=model.predict(x.astype(bool).astype(float))[0]\n",
    "    pred=prediction[position]\n",
    "    \n",
    "    indices = pred.argsort()[-k:]\n",
    "    #print(f\"{vec_to_str(x[0])} -> {vec_to_str(prediction)} -> {indices}\")\n",
    "    results=np.zeros((0, Tx, xn))\n",
    "    for index in indices:\n",
    "        res=np.array(x, copy=True)\n",
    "        res[0,position+1,index]=pred[index]\n",
    "        if index==eos_index:\n",
    "            results=np.append(results,res,axis=0)\n",
    "            break\n",
    "        if position==Tx-2:\n",
    "            results=np.append(results,res,axis=0)\n",
    "            break\n",
    "        results=np.append(results,predict_tree(res,position+1,k), axis=0)\n",
    "    return results\n",
    "\n",
    "\n",
    "def beamsearch(k, cnt):\n",
    "    x = np.zeros((1, Tx, xn)) # starting with 0\n",
    "    res=predict_tree(x, 0, k)\n",
    "    probs=[]\n",
    "    for sample in res:\n",
    "        sample_prob=1\n",
    "        for char in sample:\n",
    "            ch_prob=char.max()            \n",
    "            if ch_prob>0:\n",
    "                sample_prob*=ch_prob\n",
    "        probs.append(sample_prob)\n",
    "    samples=[vec_to_str(a) for a in np.take(res,np.argsort(probs)[-cnt:],axis=0)]\n",
    "    probs=np.take(probs,np.argsort(probs))[-cnt:]\n",
    "    return list(zip(probs,samples))\n",
    "    \n",
    "    \n",
    "res=beamsearch(k=2,cnt=10)\n",
    "for prob, sample in res:\n",
    "    print(f\"{prob:0.5f}: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48: oi a b5dddE..\n",
      "0.48: oi a bbddvE..\n",
      "1.48: oi a bbdddE..\n"
     ]
    }
   ],
   "source": [
    "for prob, sample in res:\n",
    "    print(f\"{prob:0.2f}: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "# train language model for the beam search\n",
    "# end of line character should be zeros\n",
    "# dropout\n",
    "# try GRU\n",
    "# save best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Dense, Input, LSTM,GRU\n",
    "\n",
    "drp=0\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(64, input_shape=(Tx, len(characters)),return_sequences=True,activation=\"softmax\"))\n",
    "model.add(Dense(len(characters),activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(Xf[2:3,:,:]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xf[:1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvognition",
   "language": "python",
   "name": "kvognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
